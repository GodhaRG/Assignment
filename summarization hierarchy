from sentence_transformers import SentenceTransformer


sbert_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')


embeddings = sbert_model.encode(all_chunks)
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score
from transformers import pipeline


summarizer = pipeline('summarization', model="facebook/bart-large-cnn")

def find_optimal_clusters_silhouette(embeddings, max_clusters=20):
    silhouette_scores = []
    for i in range(2, max_clusters + 1):
        gmm = GaussianMixture(n_components=i, covariance_type='tied')
        cluster_labels = gmm.fit_predict(embeddings)
        silhouette_avg = silhouette_score(embeddings, cluster_labels)
        silhouette_scores.append(silhouette_avg)
    best_n_clusters = np.argmax(silhouette_scores) + 2  # +2 because range starts from 2
    return best_n_clusters

def cluster_embeddings(embeddings, n_clusters):
    gmm = GaussianMixture(n_components=n_clusters, covariance_type='tied')
    gmm.fit(embeddings)
    return gmm.predict_proba(embeddings), gmm.means_

def summarize_texts(texts):
    summarized_texts = []
    for text in texts:
        summary = summarizer(text, max_length=150, min_length=40, do_sample=False)
        summarized_texts.append(summary[0]['summary_text'])
    return summarized_texts

def recursive_clustering_and_summarization(texts, max_depth=3, current_depth=0):
    if current_depth >= max_depth or len(texts) <= 1:
        return texts

   
    embeddings = sbert_model.encode(texts)

 
    best_n_clusters = find_optimal_clusters_silhouette(embeddings)

    
    cluster_probabilities, cluster_centers = cluster_embeddings(embeddings, best_n_clusters)

 
    cluster_texts = [[] for _ in range(best_n_clusters)]
    for idx, text in enumerate(texts):
        cluster_idx = cluster_probabilities[idx].argmax()
        cluster_texts[cluster_idx].append(text)

    summarized_texts = [summarize_texts([' '.join(cluster)]) for cluster in cluster_texts]

    
    return recursive_clustering_and_summarization(summarized_texts, max_depth, current_depth + 1)


chunk_texts = [metadata["chunk"] for metadata in metadata_db]
summarized_hierarchy = recursive_clustering_and_summarization(chunk_texts, max_depth=3)


for summary in summarized_hierarchy:
    embedding = sbert_model.encode([summary])[0]
    metadata = {"title": "Summary", "page": "N/A", "chunk": summary}
    store_in_vector_db(embedding, metadata)


for idx, summary in enumerate(summarized_hierarchy):
    print(f"Summary {idx + 1}: {summary}")
